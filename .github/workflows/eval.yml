name: Evaluate LLM
on:
  workflow_dispatch:
  pull_request:
    branches:
      - main
    paths:
      - data/*
      - src/*

permissions:
  id-token: write
  contents: read

jobs:
  evaluate:
    runs-on: ubuntu-latest
    env:
      AZURE_OPENAI_ENDPOINT: ${{ secrets.AZURE_OPENAI_ENDPOINT }}
      AZURE_OPENAI_LLM_DEPLOYMENT: ${{ secrets.AZURE_OPENAI_LLM_DEPLOYMENT }}
      AZURE_OPENAI_JUDGE_DEPLOYMENT: ${{ secrets.AZURE_OPENAI_JUDGE_DEPLOYMENT }}
      AZURE_OPENAI_API_VERSION: ${{ secrets.AZURE_OPENAI_API_VERSION }}
      # Evaluation thresholds (0.0 to 1.0 scale)
      CONCISENESS_THRESHOLD: ${{ vars.CONCISENESS_THRESHOLD || 0.7 }}
      CORRECTNESS_THRESHOLD: ${{ vars.CORRECTNESS_THRESHOLD || 0.8 }}
      HALLUCINATION_THRESHOLD: ${{ vars.HALLUCINATION_THRESHOLD || 0.9 }}
    steps:
      - uses: actions/checkout@v4
      - uses: azure/login@v2
        with:
          client-id: ${{ secrets.OIDC_AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.OIDC_AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.OIDC_AZURE_SUBSCRIPTION_ID }}

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: Install dependencies
        run: pip install -r src/requirements.txt

      - name: Run LLM Evaluation
        id: evaluation
        run: |
          echo "Running LLM evaluation..."
          python src/evaluation.py

          # Set evaluation results as output variables for easy access
          echo "results_file=evaluation_results.json" >> $GITHUB_OUTPUT

      - name: Create Job Summary from JSON
        if: always()
        run: |
          # Read the JSON results
          RESULTS=$(cat evaluation_results.json)
          
          # Extract values using jq for clean parsing
          LLM_OUTPUT=$(echo "$RESULTS" | jq -r '.llm_output')
          CONCISENESS_SCORE=$(echo "$RESULTS" | jq -r '.evaluations.conciseness.score // "N/A"')
          CONCISENESS_COMMENTS=$(echo "$RESULTS" | jq -r '.evaluations.conciseness.comments // "N/A"')
          CORRECTNESS_SCORE=$(echo "$RESULTS" | jq -r '.evaluations.correctness.score // "N/A"')
          CORRECTNESS_COMMENTS=$(echo "$RESULTS" | jq -r '.evaluations.correctness.comments // "N/A"')
          HALLUCINATION_SCORE=$(echo "$RESULTS" | jq -r '.evaluations.hallucination.score // "N/A"')
          HALLUCINATION_COMMENTS=$(echo "$RESULTS" | jq -r '.evaluations.hallucination.comments // "N/A"')
          
          # Extract threshold information
          OVERALL_PASS=$(echo "$RESULTS" | jq -r '.threshold_check.overall_pass // false')
          CONCISENESS_THRESHOLD=$(echo "$RESULTS" | jq -r '.threshold_check.thresholds.conciseness // "N/A"')
          CORRECTNESS_THRESHOLD=$(echo "$RESULTS" | jq -r '.threshold_check.thresholds.correctness // "N/A"')
          HALLUCINATION_THRESHOLD=$(echo "$RESULTS" | jq -r '.threshold_check.thresholds.hallucination // "N/A"')
          CONCISENESS_PASS=$(echo "$RESULTS" | jq -r '.threshold_check.results.conciseness.passed // false')
          CORRECTNESS_PASS=$(echo "$RESULTS" | jq -r '.threshold_check.results.correctness.passed // false')
          HALLUCINATION_PASS=$(echo "$RESULTS" | jq -r '.threshold_check.results.hallucination.passed // false')
          
          # Set status emojis
          CONCISENESS_STATUS=$([ "$CONCISENESS_PASS" = "true" ] && echo "âœ…" || echo "âŒ")
          CORRECTNESS_STATUS=$([ "$CORRECTNESS_PASS" = "true" ] && echo "âœ…" || echo "âŒ")
          HALLUCINATION_STATUS=$([ "$HALLUCINATION_PASS" = "true" ] && echo "âœ…" || echo "âŒ")
          OVERALL_STATUS=$([ "$OVERALL_PASS" = "true" ] && echo "âœ… PASSED" || echo "âŒ FAILED")

          # Create the job summary
          cat >> $GITHUB_STEP_SUMMARY << EOF
          # ğŸ¤– LLM Evaluation Results - ${OVERALL_STATUS}

          ## ğŸ¯ LLM Output
          \`\`\`
          ${LLM_OUTPUT}
          \`\`\`

          ## ğŸ“Š Evaluation Scores

          | Evaluation Type | Score | Threshold | Status | Comments |
          |----------------|-------|-----------|--------|----------|
          | ğŸ¯ Conciseness | ${CONCISENESS_SCORE} | â‰¥${CONCISENESS_THRESHOLD} | ${CONCISENESS_STATUS} | ${CONCISENESS_COMMENTS} |
          | âœ… Correctness | ${CORRECTNESS_SCORE} | â‰¥${CORRECTNESS_THRESHOLD} | ${CORRECTNESS_STATUS} | ${CORRECTNESS_COMMENTS} |
          | ğŸš« Hallucination | ${HALLUCINATION_SCORE} | â‰¥${HALLUCINATION_THRESHOLD} | ${HALLUCINATION_STATUS} | ${HALLUCINATION_COMMENTS} |

          ### ğŸ“‹ Full JSON Results
          \`\`\`json
          $(cat evaluation_results.json)
          \`\`\`
          EOF

      - name: Check Evaluation Thresholds
        if: always()
        run: |
          # Extract overall pass status
          OVERALL_PASS=$(cat evaluation_results.json | jq -r '.threshold_check.overall_pass // false')
          
          if [ "$OVERALL_PASS" = "false" ]; then
            echo "::error::âŒ Evaluation thresholds not met! This PR cannot be merged until the LLM responses meet the minimum quality standards."
            echo "::error::Please review the evaluation results and update your prompts or LLM configuration to improve the scores."
            exit 1
          else
            echo "::notice::âœ… All evaluation thresholds met! This PR passes the LLM quality checks."
          fi

      - name: Comment on PR
        if: github.event_name == 'pull_request' && always()
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const results = JSON.parse(fs.readFileSync('evaluation_results.json', 'utf8'));
            
            // Extract data
            const overallPass = results.threshold_check?.overall_pass || false;
            const llmOutput = results.llm_output || 'N/A';
            const evaluations = results.evaluations || {};
            const thresholds = results.threshold_check?.thresholds || {};
            const thresholdResults = results.threshold_check?.results || {};
            
            // Create status indicators
            const getStatusIcon = (passed) => passed ? 'âœ…' : 'âŒ';
            const getStatusText = (passed) => passed ? 'PASS' : 'FAIL';
            const overallStatus = overallPass ? 'âœ… PASSED' : 'âŒ FAILED';
            
            // Build comment
            const comment = `## ğŸ¤– LLM Evaluation Results - ${overallStatus}
            
            ### ğŸ“Š Evaluation Scores
            
            | Evaluation Type | Score | Threshold | Status | Comments |
            |----------------|-------|-----------|--------|----------|
            | ğŸ¯ Conciseness | ${evaluations.conciseness?.score || 'N/A'} | â‰¥${thresholds.conciseness || 'N/A'} | ${getStatusIcon(thresholdResults.conciseness?.passed)} ${getStatusText(thresholdResults.conciseness?.passed)} | ${evaluations.conciseness?.comments || 'N/A'} |
            | âœ… Correctness | ${evaluations.correctness?.score || 'N/A'} | â‰¥${thresholds.correctness || 'N/A'} | ${getStatusIcon(thresholdResults.correctness?.passed)} ${getStatusText(thresholdResults.correctness?.passed)} | ${evaluations.correctness?.comments || 'N/A'} |
            | ğŸš« Hallucination | ${evaluations.hallucination?.score || 'N/A'} | â‰¥${thresholds.hallucination || 'N/A'} | ${getStatusIcon(thresholdResults.hallucination?.passed)} ${getStatusText(thresholdResults.hallucination?.passed)} | ${evaluations.hallucination?.comments || 'N/A'} |
            
            ### ğŸ¯ LLM Output
            \`\`\`
            ${llmOutput}
            \`\`\`
            
            ${overallPass ? 
              'ğŸ‰ **All evaluations passed!** This PR meets the minimum quality thresholds and can be merged.' : 
              'âš ï¸ **Some evaluations failed!** Please review the results above and improve the LLM responses before merging this PR.'
            }
            
            ---
            *Evaluation run: ${context.runId} | Commit: ${context.sha.substring(0, 7)}*`;
            
            // Post comment
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });

      - name: Upload Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-results
          path: evaluation_results.json
          retention-days: 30