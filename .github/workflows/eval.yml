name: Evaluate LLM
on:
  workflow_dispatch:
  pull_request:
    branches:
      - main
    paths:
      - data/*
      - src/*

permissions:
  id-token: write
  contents: read

jobs:
  evaluate:
    runs-on: ubuntu-latest
    env:
      AZURE_OPENAI_ENDPOINT: ${{ secrets.AZURE_OPENAI_ENDPOINT }}
      AZURE_OPENAI_LLM_DEPLOYMENT: ${{ secrets.AZURE_OPENAI_LLM_DEPLOYMENT }}
      AZURE_OPENAI_JUDGE_DEPLOYMENT: ${{ secrets.AZURE_OPENAI_JUDGE_DEPLOYMENT }}
      AZURE_OPENAI_API_VERSION: ${{ secrets.AZURE_OPENAI_API_VERSION }}
      # GENAI_EVALS_CONFIG_PATH: ${{ github.workspace }}/evaluate-config.json
      # GENAI_EVALS_DATA_PATH: ${{ github.workspace }}/.github/.test_files/eval-input.jsonl
    steps:
      - uses: actions/checkout@v4
      - uses: azure/login@v2
        with:
          client-id: ${{ secrets.OIDC_AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.OIDC_AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.OIDC_AZURE_SUBSCRIPTION_ID }}

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: Install dependencies
        run: pip install -r src/requirements.txt

      - name: Run LLM Evaluation
        id: evaluation
        run: |
          echo "Running LLM evaluation..."
          python src/evaluation.py

          # Set evaluation results as output variables for easy access
          echo "results_file=evaluation_results.json" >> $GITHUB_OUTPUT

      - name: Create Job Summary from JSON
        if: success()
        run: |
          # Read the JSON results
          RESULTS=$(cat evaluation_results.json)          # Extract values using jq for clean parsing
          LLM_OUTPUT=$(echo "$RESULTS" | jq -r '.llm_output')
          CONCISENESS_SCORE=$(echo "$RESULTS" | jq -r '.evaluations.conciseness.score // "N/A"')
          CONCISENESS_COMMENTS=$(echo "$RESULTS" | jq -r '.evaluations.conciseness.comments // "N/A"' | head -c 100)
          CORRECTNESS_SCORE=$(echo "$RESULTS" | jq -r '.evaluations.correctness.score // "N/A"')
          CORRECTNESS_COMMENTS=$(echo "$RESULTS" | jq -r '.evaluations.correctness.comments // "N/A"' | head -c 100)
          HALLUCINATION_SCORE=$(echo "$RESULTS" | jq -r '.evaluations.hallucination.score // "N/A"')
          HALLUCINATION_COMMENTS=$(echo "$RESULTS" | jq -r '.evaluations.hallucination.comments // "N/A"' | head -c 100)

          # Create the job summary
          cat >> $GITHUB_STEP_SUMMARY << EOF
          # ðŸ¤– LLM Evaluation Results

          ## ðŸŽ¯ LLM Output
          \`\`\`
          ${LLM_OUTPUT}
          \`\`\`

          ## ðŸ“Š Evaluation Scores

          | Evaluation Type | Score | Comments |
          |----------------|-------|----------|
          | ðŸŽ¯ Conciseness | ${CONCISENESS_SCORE} | ${CONCISENESS_COMMENTS} |
          | âœ… Correctness | ${CORRECTNESS_SCORE} | ${CORRECTNESS_COMMENTS} |
          | ðŸš« Hallucination | ${HALLUCINATION_SCORE} | ${HALLUCINATION_COMMENTS} |

          ### ðŸ“‹ Full JSON Results
          \`\`\`json
          $(cat evaluation_results.json)
          \`\`\`
          EOF

      - name: Upload Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-results
          path: evaluation_results.json
          retention-days: 30