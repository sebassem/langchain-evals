name: Evaluate LLM
on:
  workflow_dispatch:
  pull_request:
    branches:
      - main
    paths:
      - data/*
      - src/*

permissions:
  id-token: write
  contents: read

jobs:
  evaluate:
    runs-on: ubuntu-latest
    env:
      AZURE_OPENAI_ENDPOINT: ${{ secrets.AZURE_OPENAI_ENDPOINT }}
      AZURE_OPENAI_LLM_DEPLOYMENT: ${{ secrets.AZURE_OPENAI_LLM_DEPLOYMENT }}
      AZURE_OPENAI_JUDGE_DEPLOYMENT: ${{ secrets.AZURE_OPENAI_JUDGE_DEPLOYMENT }}
      AZURE_OPENAI_API_VERSION: ${{ secrets.AZURE_OPENAI_API_VERSION }}
      # Evaluation thresholds (0.0 to 1.0 scale)
      CONCISENESS_THRESHOLD: ${{ vars.CONCISENESS_THRESHOLD || 0.7 }}
      CORRECTNESS_THRESHOLD: ${{ vars.CORRECTNESS_THRESHOLD || 0.8 }}
      HALLUCINATION_THRESHOLD: ${{ vars.HALLUCINATION_THRESHOLD || 0.9 }}
    steps:
      - uses: actions/checkout@v4
      - uses: azure/login@v2
        with:
          client-id: ${{ secrets.OIDC_AZURE_CLIENT_ID }}
          tenant-id: ${{ secrets.OIDC_AZURE_TENANT_ID }}
          subscription-id: ${{ secrets.OIDC_AZURE_SUBSCRIPTION_ID }}

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.x'

      - name: Install dependencies
        run: pip install -r src/requirements.txt

      - name: Run LLM Evaluation
        id: evaluation
        run: |
          echo "Running LLM evaluation..."
          python src/evaluation.py

          # Set evaluation results as output variables for easy access
          echo "results_file=evaluation_results.json" >> $GITHUB_OUTPUT

      - name: Create Job Summary from JSON
        if: always()
        run: |
          # Read the JSON results
          RESULTS=$(cat evaluation_results.json)

          # Extract values using jq for clean parsing
          LLM_OUTPUT=$(echo "$RESULTS" | jq -r '.llm_output')
          CONCISENESS_SCORE=$(echo "$RESULTS" | jq -r '.evaluations.conciseness.score // "N/A"')
          CONCISENESS_COMMENTS=$(echo "$RESULTS" | jq -r '.evaluations.conciseness.comments // "N/A"')
          CORRECTNESS_SCORE=$(echo "$RESULTS" | jq -r '.evaluations.correctness.score // "N/A"')
          CORRECTNESS_COMMENTS=$(echo "$RESULTS" | jq -r '.evaluations.correctness.comments // "N/A"')
          HALLUCINATION_SCORE=$(echo "$RESULTS" | jq -r '.evaluations.hallucination.score // "N/A"')
          HALLUCINATION_COMMENTS=$(echo "$RESULTS" | jq -r '.evaluations.hallucination.comments // "N/A"')

          # Extract threshold information
          OVERALL_PASS=$(echo "$RESULTS" | jq -r '.threshold_check.overall_pass // false')
          CONCISENESS_THRESHOLD=$(echo "$RESULTS" | jq -r '.threshold_check.thresholds.conciseness // "N/A"')
          CORRECTNESS_THRESHOLD=$(echo "$RESULTS" | jq -r '.threshold_check.thresholds.correctness // "N/A"')
          HALLUCINATION_THRESHOLD=$(echo "$RESULTS" | jq -r '.threshold_check.thresholds.hallucination // "N/A"')
          CONCISENESS_PASS=$(echo "$RESULTS" | jq -r '.threshold_check.results.conciseness.passed // false')
          CORRECTNESS_PASS=$(echo "$RESULTS" | jq -r '.threshold_check.results.correctness.passed // false')
          HALLUCINATION_PASS=$(echo "$RESULTS" | jq -r '.threshold_check.results.hallucination.passed // false')

          # Set status emojis
          CONCISENESS_STATUS=$([ "$CONCISENESS_PASS" = "true" ] && echo "✅" || echo "❌")
          CORRECTNESS_STATUS=$([ "$CORRECTNESS_PASS" = "true" ] && echo "✅" || echo "❌")
          HALLUCINATION_STATUS=$([ "$HALLUCINATION_PASS" = "true" ] && echo "✅" || echo "❌")
          OVERALL_STATUS=$([ "$OVERALL_PASS" = "true" ] && echo "✅ PASSED" || echo "❌ FAILED")

          # Create the job summary
          cat >> $GITHUB_STEP_SUMMARY << EOF
          # 🤖 LLM Evaluation Results - ${OVERALL_STATUS}

          ## 🎯 LLM Output
          \`\`\`
          ${LLM_OUTPUT}
          \`\`\`

          ## 📊 Evaluation Scores

          | Evaluation Type | Score | Threshold | Status | Comments |
          |----------------|-------|-----------|--------|----------|
          | 🎯 Conciseness | ${CONCISENESS_SCORE} | ≥${CONCISENESS_THRESHOLD} | ${CONCISENESS_STATUS} | ${CONCISENESS_COMMENTS} |
          | ✅ Correctness | ${CORRECTNESS_SCORE} | ≥${CORRECTNESS_THRESHOLD} | ${CORRECTNESS_STATUS} | ${CORRECTNESS_COMMENTS} |
          | 🚫 Hallucination | ${HALLUCINATION_SCORE} | ≥${HALLUCINATION_THRESHOLD} | ${HALLUCINATION_STATUS} | ${HALLUCINATION_COMMENTS} |

          ### 📋 Full JSON Results
          \`\`\`json
          $(cat evaluation_results.json)
          \`\`\`
          EOF

      - name: Check Evaluation Thresholds
        if: always()
        run: |
          # Extract overall pass status
          OVERALL_PASS=$(cat evaluation_results.json | jq -r '.threshold_check.overall_pass // false')

          if [ "$OVERALL_PASS" = "false" ]; then
            echo "::error::❌ Evaluation thresholds not met! This PR cannot be merged until the LLM responses meet the minimum quality standards."
            echo "::error::Please review the evaluation results and update your prompts or LLM configuration to improve the scores."
            exit 1
          else
            echo "::notice::✅ All evaluation thresholds met! This PR passes the LLM quality checks."
          fi

      - name: Upload Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: evaluation-results
          path: evaluation_results.json
          retention-days: 30